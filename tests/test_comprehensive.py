"""
Comprehensive test suite for all implemented conversational AI features.
Tests every feature mentioned in the implementation to ensure they work as intended.
"""

import sys
import os
import pytest
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


def test_natural_language_summaries():
    """Test natural language SQL result summaries"""
    try:
        from backend.query_engine import summarize_sql_result_natural_language
    except ImportError:
        pytest.skip("Skipping test due to missing dependencies")

    try:
        # Test with sample SQL result
        sample_result = {
            "rows": [
                {"category": "electronics", "sales": 15000},
                {"category": "books", "sales": 8000},
                {"category": "clothing", "sales": 12000},
            ]
        }

        summary = summarize_sql_result_natural_language(
            "Show top selling categories", sample_result
        )
        assert isinstance(summary, str), "Summary should be a string"
        assert len(summary) > 10, "Summary should be meaningful"
        print(f"‚úì Natural language summary: {summary[:100]}...")
    except Exception as e:
        print(f"‚úó Natural language summaries failed: {e}")
        raise


def test_multilingual_support():
    """Test multilingual query processing"""
    from backend.features import detect_language, translate_to_english
    from backend.query_engine import QueryEngine

    # Test Portuguese query
    portuguese_query = "Quais s√£o as categorias mais vendidas?"
    lang = detect_language(portuguese_query)
    assert lang == "pt", f"Expected Portuguese detection, got {lang}"

    translated = translate_to_english(portuguese_query)
    assert (
        "categories" in translated.lower() or "vendidas" not in translated.lower()
    ), "Should translate to English"

    # Test query engine with translated query
    engine = QueryEngine()
    result = engine.execute_query(translated)
    assert result is not None, "Query should execute successfully"

    print(f"‚úì Multilingual support: '{portuguese_query}' -> '{translated}'")


def test_multi_turn_reasoning():
    """Test follow-up question detection and context maintenance"""
    from backend.query_engine import QueryEngine
    from backend.utils.session_memory import detect_follow_up

    # Test follow-up detection
    initial_query = "Show top selling categories"
    follow_up = "What about last month?"

    is_follow_up, enriched = detect_follow_up(follow_up, initial_query)
    assert is_follow_up == True, "Should detect as follow-up"

    # Test with query engine
    engine = QueryEngine(session_id="test_multi_turn")

    # First query
    result1 = engine.execute_query(initial_query)
    assert result1 is not None, "First query should work"

    # Follow-up query
    result2 = engine.execute_query(follow_up)
    assert result2 is not None, "Follow-up query should work"

    # Check memory
    memory = engine.memory
    history = memory.get_context_string("test_multi_turn")
    assert len(history) > 0, "Should maintain conversation history"

    print(f"‚úì Multi-turn reasoning: Follow-up detected, history length: {len(history)}")


def test_sentiment_emotion_analysis():
    """Test sentiment and emotion analysis in reviews"""
    try:
        from backend.features import analyze_sentiment_emotion

        # Test positive review
        positive_review = "This product is amazing! Fast delivery and great quality."
        result = analyze_sentiment_emotion(positive_review)

        assert "sentiment" in result, "Should have sentiment"
        assert "emotion" in result, "Should have emotion"
        assert result["sentiment"] in [
            "POSITIVE",
            "NEGATIVE",
            "NEUTRAL",
        ], "Sentiment should be valid"

        # Test negative review
        negative_review = "Terrible product, arrived broken and poor customer service."
        result2 = analyze_sentiment_emotion(negative_review)
        assert result2["sentiment"] in [
            "POSITIVE",
            "NEGATIVE",
            "NEUTRAL",
        ], "Sentiment should be valid"

        print(
            f"‚úì Sentiment analysis: Positive='{result['sentiment']}', Negative='{result2['sentiment']}'"
        )
    except Exception as e:
        print(f"‚úó Sentiment analysis failed: {e}")
        raise


def test_sql_generation_and_validation():
    """Test SQL generation with context and validation"""
    try:
        from backend.features import generate_sql
        from backend.sql_executor import validate_sql
    except ImportError as e:
        pytest.skip(f"Skipping test due to missing dependencies: {e}")

        # Test SQL generation with context
        query = "Show sales by category for electronics"
        context = "Previous query was about product categories"

        result = generate_sql(query, context)
        assert "sql" in result, "Should generate SQL"
        assert "path" in result, "Should save to file"

        sql = result["sql"]
        is_valid = validate_sql(sql)
        assert is_valid, f"Generated SQL should be valid: {sql}"

        print(f"‚úì SQL generation: Valid SQL generated ({len(sql)} chars)")
    except Exception as e:
        print(f"‚úó SQL generation failed: {e}")
        raise


def test_code_generation():
    """Test Python code generation for analysis"""
    from backend.features import generate_code

    query = "Create a chart showing sales trends over time"
    result = generate_code(query)

    # generate_code returns just the code string, not a dict
    assert isinstance(result, str), "Should return string"
    assert len(result) > 10, "Should return some content"

    # Check if we got actual code or an error message
    if "Error" in result or "Syntax error" in result:
        # LLM providers failed, skip the detailed checks
        pytest.skip("LLM providers unavailable for code generation testing")
    else:
        # We got actual code, check it contains expected elements
        assert len(result) > 100, "Should generate substantial code"
        assert "import" in result or "def " in result, "Should contain Python code"
        assert (
            "matplotlib" in result or "plt" in result
        ), "Should include plotting for chart request"

        print(f"‚úì Code generation: Generated {len(result)} chars of code")


def test_error_handling_and_validation():
    """Test error handling and input validation"""
    try:
        from backend.utils.error_handler import handle_errors
        from backend.query_engine import QueryEngine

        # Test error handler decorator
        @handle_errors(default_return="Error handled", log_level="warning")
        def failing_function():
            raise ValueError("Test error")

        result = failing_function()
        assert result == "Error handled", "Should return default on error"

        # Test query engine with invalid input
        engine = QueryEngine()
        result = engine.execute_query("")  # Empty query
        assert result is not None, "Should handle empty queries gracefully"

        print("‚úì Error handling: Graceful error handling and validation")
    except Exception as e:
        print(f"‚úó Error handling failed: {e}")
        raise


def test_llm_token_limits():
    """Test that LLM calls use appropriate token limits"""
    try:
        from backend.llm_config import LLMTokenLimits, get_token_limit

        # Test token limit constants
        assert LLMTokenLimits.SHORT == 150, "Short limit should be 150"
        assert LLMTokenLimits.SQL == 400, "SQL limit should be 400"
        assert LLMTokenLimits.CODE == 800, "Code limit should be 800"

        # Test get_token_limit function
        assert get_token_limit("summary") == 150, "Summary should use SHORT limit"
        assert get_token_limit("sql") == 400, "SQL should use SQL limit"
        assert get_token_limit("unknown") == 256, "Unknown should use DEFAULT"

        print("‚úì LLM token limits: All limits configured correctly")
    except Exception as e:
        print(f"‚úó LLM token limits failed: {e}")
        raise


def test_hybrid_query_processing():
    """Test hybrid SQL + RAG query processing"""
    try:
        from backend.query_engine import QueryEngine

        engine = QueryEngine(hybrid_mode=True)

        # Test hybrid query
        query = "What do customers say about delivery times for electronics?"
        result = engine.execute_query(query)

        assert result is not None, "Hybrid query should execute"
        assert "type" in result, "Should have result type"

        # Check for both SQL and RAG components if present
        has_sql = "sql_result" in result
        has_rag = "rag_result" in result

        if has_sql or has_rag:
            print(f"‚úì Hybrid processing: SQL={has_sql}, RAG={has_rag}")
        else:
            print("‚úì Hybrid processing: Query executed successfully")
    except Exception as e:
        print(f"‚úó Hybrid processing failed: {e}")
        raise


if __name__ == "__main__":
    print("=" * 80)
    print("COMPREHENSIVE FEATURE TESTING SUITE")
    print("=" * 80)
    print("Testing all implemented conversational AI features...")
    print()

    # Define all tests
    tests = [
        ("Natural Language Summaries", test_natural_language_summaries),
        ("Multilingual Support", test_multilingual_support),
        ("Multi-turn Reasoning", test_multi_turn_reasoning),
        ("Sentiment & Emotion Analysis", test_sentiment_emotion_analysis),
        ("SQL Generation & Validation", test_sql_generation_and_validation),
        ("Code Generation", test_code_generation),
        ("Error Handling & Validation", test_error_handling_and_validation),
        ("LLM Token Limits", test_llm_token_limits),
        ("Hybrid Query Processing", test_hybrid_query_processing),
    ]

    passed = 0
    failed = 0

    for name, test_func in tests:
        print(f"\nüß™ {name}:")
        print("-" * 60)
        try:
            if test_func():
                passed += 1
                print(f"‚úÖ {name} - PASSED")
            else:
                failed += 1
                print(f"‚ùå {name} - FAILED")
        except Exception as e:
            failed += 1
            print(f"üí• {name} - CRASHED: {e}")

    print("\n" + "=" * 80)
    print("FINAL RESULTS")
    print("=" * 80)
    print(f"‚úÖ PASSED: {passed}")
    print(f"‚ùå FAILED: {failed}")
    print(
        f"üìä SUCCESS RATE: {passed}/{passed + failed} ({(passed/(passed+failed)*100):.1f}%)"
    )

    if failed == 0:
        print("\nüéâ ALL FEATURES WORKING AS INTENDED!")
        print("üöÄ Your conversational AI system is fully functional!")
    else:
        print(f"\n‚ö†Ô∏è  {failed} feature(s) need attention.")

    print("=" * 80)
